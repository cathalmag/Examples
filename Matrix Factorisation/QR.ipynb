{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QR Factorisation\n",
    "\n",
    "We will look at two different ways for computing the QR factorisation of a matrix. To goal is to start from a matrix $A$ and write it as the product of an orthogonal matrix $Q$ and an upper-triangular matrix $R$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "\n",
    "A = np.array([[1,2,3],[4,5,6],[7,8,10]])\n",
    "b = np.array([1,2,3])\n",
    "n = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numpy has a built-in function for doing this. Let's use it to check what the answer should be:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gram-Schmidt orthogonalization\n",
    "\n",
    "The first approach will be to transform the vectors in the columns of $A$ to a set of orthogonal vectores using the Gram-Schmidt approach. The basic idea of Gram-Schmidt is to build up an orthonormal set of vectors by projecting out non-orthogonal pieces. The following image illustrates this.\n",
    "![Gram-Schmidt Visualisation](Gram-Schmidt_orthonormalization_process.gif \"Gram-Schmidt Visualisation\")\n",
    "Let's now implement this with our test matrix $A$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we construct three vectors $a_1$, $a_2$ and $a_3$ from the columns of $A$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "78"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(a1,a2,a3)=np.transpose(A)\n",
    "a1@a2\n",
    "# Not zero therefore not orthogonal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, our first orthonormal vector is just $a_1$ normalised to have length 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "u1=a1\n",
    "e1=u1/norm(u1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To construct our second orthonormal vector, let's start with $a_2$, project out the part along the $a_1$ direction and normalise the result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "u2=a2-(e1@a2)*e1\n",
    "e2=u2/norm(u2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also project this piece out from $a_3$ now (this is not essential, but helps improve the numerical stability of the algorithm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "u3=a3-(e1@a3)*e1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To construct our third orthonormal vector, let's project out the part along the previous two directions and normalise the result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "u3=u3-(e2@u3)*e2\n",
    "e3=u3/norm(u3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have our three orthogonal vectors, we can put them into the columns of Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.12309149,  0.90453403,  0.40824829],\n",
       "       [ 0.49236596,  0.30151134, -0.81649658],\n",
       "       [ 0.86164044, -0.30151134,  0.40824829]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q=np.transpose([e1,e2,e3])\n",
    "Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get $R$, we note that $A = Q R$ means that $Q^T A = Q^T Q R = R$ since $Q$ is an orthogonal matrix. Let's use this to compute $R$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[8.12403840e+00, 9.60113630e+00, 1.19398746e+01],\n",
       "       [1.19348975e-14, 9.04534034e-01, 1.50755672e+00],\n",
       "       [1.55986335e-14, 1.86517468e-14, 4.08248290e-01]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Q is orthogonal therefore R is Q inverse times A which is Q transpose times A\n",
    "R=np.transpose(Q)@A\n",
    "R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, $R$ is (almost) an upper-triangular matrix. It is only __almost__ upper triangular because floating point arithmetic is not exact."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Householder reflections\n",
    "\n",
    "The Gram-Schmidt process can be a very effective way to orthogonalise a set of vectors, but it does run into problems with numerical stability. This can happen, for example, in the case where we are starting with vectors that are nearly linearly dependent. Then we would be subtracting two large vectors to produce one small one, and we know that this is a recipe for disaster with floating point arithmetic.\n",
    "\n",
    "One way around this problem is to use a different approach to orthogonalization. A very popular method uses the idea of Householder reflections. These take a vector $x$ and reflect it about a plane defined by another vector $v$:\n",
    "![Householder reflection](Householder.png \"Householder reflection\")\n",
    "This is clearly equivalent to multiplying $x$ by the __Householder reflection matrix__\n",
    "$$ H = I - 2 \\frac{v v^T}{||v||^2}$$\n",
    "Note that $H$ is a *symmetric, orthogonal matrix*.\n",
    "\n",
    "Now, if we choose $v$ appropriately then we can use it to zero out below the pivot in each column, thus producing $R$. In particular, if\n",
    "$$v = a - ||a|| e_k$$ then $H a = |a| e_k$ so if $e_k$ is a unit vector in the $k$-th direction this does exactly what we want to the column $a$.\n",
    "\n",
    "Let's now implement this in practice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we work on the first column of $A$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.12309149, -0.49236596, -0.86164044],\n",
       "       [-0.49236596,  0.7841456 , -0.3777452 ],\n",
       "       [-0.86164044, -0.3777452 ,  0.33894589]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u1=a1-(-np.sign(a1[0]))*norm(a1)*np.array([1,0,0])\n",
    "v1=u1/norm(u1)\n",
    "H1=np.identity(n)-2*np.outer(v1,v1)\n",
    "H1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-8.12403840e+00, -9.60113630e+00, -1.19398746e+01],\n",
       "       [ 7.21644966e-16, -8.59655700e-02, -5.49676344e-01],\n",
       "       [ 7.77156117e-16, -9.00439748e-01, -1.46193360e+00]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A1=H1@A\n",
    "A1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and the second column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "a2=A1[1:,1]\n",
    "u2=a2-(-np.sign(a2[0]))*norm(a2)*np.array([1,0])\n",
    "v2=u2/norm(u2)\n",
    "H2=np.identity(n)\n",
    "H2[1:,1:]-=2*np.outer(v2,v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-8.12403840e+00, -9.60113630e+00, -1.19398746e+01],\n",
       "       [-8.42222460e-16,  9.04534034e-01,  1.50755672e+00],\n",
       "       [-6.44518747e-16,  2.17887053e-17,  4.08248290e-01]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A2=H2@A1\n",
    "A2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and finally the third column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a3=A2[2:,2]\n",
    "u3=a3-(-np.sign(a3[0]))*norm(a3)*np.array([1])\n",
    "v3=u3/norm(u3)\n",
    "H3=np.identity(n)\n",
    "H3[2:,3:]-=2*np.outer(v3,v3)\n",
    "H3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-8.12403840e+00, -9.60113630e+00, -1.19398746e+01],\n",
       "       [-8.42222460e-16,  9.04534034e-01,  1.50755672e+00],\n",
       "       [-6.44518747e-16,  2.17887053e-17,  4.08248290e-01]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A3=H3@A2\n",
    "A3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have transformed to exactly $R$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[8.12403840e+00, 9.60113630e+00, 1.19398746e+01],\n",
       "       [1.19348975e-14, 9.04534034e-01, 1.50755672e+00],\n",
       "       [1.55986335e-14, 1.86517468e-14, 4.08248290e-01]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can easily construct $Q$ from the Householder matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.12309149,  0.90453403,  0.40824829],\n",
       "       [ 0.49236596,  0.30151134, -0.81649658],\n",
       "       [ 0.86164044, -0.30151134,  0.40824829]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.12309149,  0.90453403,  0.40824829],\n",
       "       [-0.49236596,  0.30151134, -0.81649658],\n",
       "       [-0.86164044, -0.30151134,  0.40824829]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# R = H3H2H1A\n",
    "# Q tarnspose = H3H2H1\n",
    "# Q = H1 transpose times H2 tranpose times H3 tranpose = H1H2H3\n",
    "np.transpose(H3@H2@H1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
